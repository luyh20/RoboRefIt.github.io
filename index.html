<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="RoboRefIt">
  <meta name="keywords" content="RoboRefIt, Visual Grounding, Robot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RoboRefIt</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');



  </script>
  
  
	<style>
       .div_photo{
               height:300px;
               width:40%;
               float:left;
               margin-left:30px;
               margin-top:10px;
               margin-bottom: 30px;
        }
       .div_phototime{
               height:25px;
               width:40%;
               text-align:center;
               line-height:25px;
               position: absolute;
        }
      .div_body{
        width:100%;
        height:800px;
        margin-top:10px;
      }
	</style> 

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
  
<body>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RoboRefIt </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yuhao Lu,</span>
            <span class="author-block">
              Yixuan Fan,</span>
            <span class="author-block">
              Beixing Deng,
            </span>
            <span class="author-block">
              Fangfu Liu,
            </span>
            <span class="author-block">
              Yali Li,
            </span>
            <span class="author-block">
              Shengjin Wang
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Department of Electronic Engineering, </span>
            <span class="author-block">Tsinghua University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              

 
              <!-- Video Link. -->
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/luyh20/RoboRefIt"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/u/0/uc?id=1-kEfagTovmcbEFG6-C_xatZfFiLWQmiU&export=download"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multi-model learning of vision-and-language is beneficial for improving the robotic interaction and perception 
            ability under complex environments. Visual grounding is an emerging vision-and-language task that aims to find 
            the unique target in the picture by a referring expression. Some current researches of voice interactive robot 
            manipulation have attempted to integrate the visual grounding task into the robot operation engine. However, 
            existing visual grounding datasets are hard to be used as a suitable practical test bed for human-robot 
            interaction because of their low correlation with robotics. In the work of the <b>VL-Grasp</b>, we contribute a new challenging 
            visual grounding dataset for robotic perception and reasoning in indoor environments, called <b>RoboRefIt</b>. 
            The RoboRefIt collects 10,872 real-world RGB and depth images from cluttered daily life scenes, and generates 
            50,758 referring expressions in the form of robot language instructions. Moreover, nearly half of the images 
            involve ambiguous object recognition. 
            We hope that the RoboRefIt provides a distinctive training bed of visual grounding tasks
            for the robot interactive grasp.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Overview. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Overview</h2>

        <div class="content has-text-justified">
          <p>
            The RoboRefIt dataset is proposed in the work of the <b>VL-Grasp</b>.
            The RoboRefIt dataset is a new challenging visual grounding dataset and is specially designed for the robot interactive grasp task.
            Data collection, data annotation and datasets comparison details can be found in the <b>VL-Grasp</b>. The content of this homepage
            is mainly used as a supplement to the VL-Grasp paper, introducing the dataset statistics, datasheets, data format, and so on.
        </div>
      </div>
    </div>
    <!--/ Overview. -->
    
    <!-- Dataset Statistics. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Dataset Statistics for the RoboRefIt</h2>

        <div class="content has-text-justified">
        
          <p>
            We make statistics on the some main data components of the RoboRefIt dataset. First, in the aspect of text content,
            the average length of the sentences is 9.5 and the detailed distribution of sentence lengths is shown in <b>Figure1</b>.
            <b>Figure2</b> reflects the frequency of words or phrases in expressions according to the size. 

            
            Second, in the aspect of object selection, the RoboRefIt dataset
            contains 66 categories of objects and the distribution of number of instances per category is shown in <b>Figure3</b>.
           
            <b>Figure4</b> shows the cumulative distribution curve of the size of the bounding boxes and 
            the masks, where a large number of objects have a pixel area from 10<sup> 3</sup> to 10<sup> 4</sup>. 
            Third, in the aspect of scene collection, the objects of the RoboRefIt are distributed in six life scenes as is shown in <b>Figure5</b>. 
            Further, for example, there are 24,521 objects collected from the table scene, while these tables are also diverse.
            
          </p>
          <div>
                      <div class="div_photo"><img src="static/images/sentence_len.png" /><div  class="div_phototime" >Figure1</div></div>
                      <div class="div_photo"><img src="static/images/wordcloud.png" /><div  class="div_phototime">Figure2</div></div>
                      <div class="column is-full-width">
                      <img src="static/images/cat.png" height="500" width="800"/><div class="columns is-centered">Figure3</div>
                      </div>     
                  </div>
          <div>
            <div class="column is-full-width"></div>
            <div class="div_photo"><img src="static/images/bbox_mask_size.png" /><div  class="div_phototime">Figure4</div></div>
            <div class="div_photo"><img src="static/images/scene_distribution.png" /><div  class="div_phototime">Figure5</div></div>   
          </div>

        </div>
      </div>
    </div>
    <!--/ Dataset Statistics. -->
    
    
    <!-- Datasheets. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Datasheets for the RoboRefIt</h2>

        <div class="content has-text-justified">
          <p>
            The RoboRefIt comprises RGB images, depth images, expressions, masks, bounding boxes.
            Each RGB image corresponds to a depth image, and multiple expressions, masks, bounding boxes. 
            Each expression describes an object in the image and corresponds one mask and one bounding box.
          </p>
          <p>
            The RoboRefIt comprises 10,872 RGB-D images and 50,758 language expressions. 
            There are 204 instances of objects involving 66 categories. These objects present at the images as the targets.
            You can understand these objects more clearly by the
            <a href="https://drive.google.com/u/0/uc?id=1WgOnBFogPedgcO5oDaO3-ZTAdGsyGIhq&export=download">Object list.</a>
          </p>
        </div>
      </div>
    </div>
    <!--/ Datasheets. -->

    <!-- Formats. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Data Format</h2>

        <div class="content has-text-justified">
          <p>
            Download the RoboRefIt! Unzip the file in RoboRefIt/data/final_dataset/. There are train set, testA set and testB set.
          </p>
          <p>
            Each json file describe the full information to load instances of these sets.
            The json files contains:
            <ol>
            <li>"num", the number of the instance.
            <li>"text", the referring expression of the instance to describe the instance.
            <li>"bbox", the vertex coordinates of the upper left corner and the vertex coordinates of the lower right corner of the bounding box of the instance.
            <li>"rgb_path", the path of related RGB image for the text. For example, "final_dataset\\train\\image\\0000000.png".
            <li>"depth_path", the path of related depth image for the text. For example, "final_dataset\\train\\depth\\0000000.png".
            <li>"mask_path", the path of related mask image for the text. For example, "final_dataset\\train\\mask\\0000000.png".
            <li>"scene", the scene type of the image.
            <li>"class", the object type of the instance.
            </ol>
          </p>
          <p>
            Moreover, there is a "depth_colormap" file, which store corresponding color map of the depth image.
          </p>
        </div>
      </div>
    </div>
    <!--/ Formats. -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
